{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a href=https://gym.openai.com>OpenAI Gym</a> это набор инструментов для разработки и сравнения алгоритмов обучения с подкреплением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym предоставляет простой и универсальный API ко многим средам с разными свойствами, как простым так и сложным:\n",
    "* Классические задачи управления и игрушечные примеры, которые можно найти в учебниках и на которых демонстрируется работа алгоритмов обучения с подкреплением (в основном они будут использоваться в этом курсе)\n",
    "* Игры Atari (оказали огромное влияние на достижения в обучении с подкреплением в последние годы)\n",
    "* 2D и 3D среды для контроля роботов в симуляции (используют проприетарный движок <a href=http://www.mujoco.org/>MuJoCo</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим агента со случайными действиями в среде <a href=https://gym.openai.com/envs/CartPole-v0>CartPole-v0</a>. В этой среде агент должен удержать шест, который закреплен на тележке, в вертикальном положении. Агент может прикладывать силу *+1* или *-1* к тележке в попытке удержать шест. Эпизод игры заканчивается, если шест отклоняется от вертикального положения больше, чем на *15* градусов или тележка сдвигается больше, чем на *2.4* единицы расстояния от центра. За каждый шаг времени, когда шест находится в вертикальном положении, агент получает награду в размере *+1* очко. Задаче считается \"решенной\" при получении агентом средней за 100 попыток награды *195* очков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для работы нам потребуется установить gym и numpy. Gym следует установить командой pip install gym=0.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0') # создаем среду CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # агент выбирает случайные действия\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # выключим визуализацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним элементы проблемы обучения с подкреплением <img src=\"./scheme.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym предоставляет такой же интерфейс взаимодействия со средой:\n",
    "* Среда в ответ на действие агента предоставляет *observation (object)*- специфичный для конкретной среды объект, который предствляет наблюдения агента. Например, пиксели камеры, значения углов и скоростей сочленений робота или позиции агента и других объектов в среде.\n",
    "* *reward (float)* - значение награды, полученной агентом в резултате совершенного действия\n",
    "* *done (boolean)* - флаг обозначающий окончание эпизода. Например, эпизод заканчивается, когда шест слишком сильно отклонился или агент попал в прорубь в среде FrozenLake\n",
    "* *info (dict)* - словарь, содержащий диагностическую информацию, которую можно использовать для отладки, но не для обучения агента. Обычно мы присваеваем значение *info* переменной по-умолчанию *_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы познакомились с API OpenAI Gym, посмотрим, сколько очков награды сможет в среднем получить за 100 эпизодов агент, выбирающий случайные действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = []\n",
    "env = gym.make('CartPole-v0')\n",
    "for episode in range(100):\n",
    "    episode_reward = 0\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            print(\"Episode {} finished after {} timesteps\".format(episode+1, t+1))\n",
    "            break\n",
    "    total_reward.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.42\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наш \"cлучайный\" агент получает в среднем 20 очков за 100 эпизодов. Не очень впечатляет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущием эксперименте агент выбирал случайное действие. Важными объектами в OpenAI Gym являются пространства состояний и действий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A discrete space in :math:`\\{ 0, 1, \\dots, n-1 \\}`. \n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> Discrete(2)\n",
      "        \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A box in R^n, i.e.each coordinate is bounded.\n",
      "    \n",
      "    There are two common use cases:\n",
      "    \n",
      "    * Identical bound for each dimension::\n",
      "        >>> Box(low=-1.0, high=2.0, shape=(3, 4), dtype=np.float32)\n",
      "        Box(3, 4)\n",
      "        \n",
      "    * Independent bound for each dimension::\n",
      "        >>> Box(low=np.array([-1.0, -2.0]), high=np.array([2.0, 4.0]), dtype=np.float32)\n",
      "        Box(2,)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На лекции мы рассмотрели, как мы можем выучить оптимальную политику, используя алгоритм Value Iteration, если нам известна динамика среды, а также если пространства состояний и действий не большие и дискретные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем выучить оптимальную политику в среде <a href=https://gym.openai.com/envs/FrozenLake-v0>FrozenLake-v0</a>. Это простая среда с маленькими пространствами состояний и действий, а также с известной динамикой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Созданим среду и выведем её описание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
      "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
      "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
      "    If you step into one of those holes, you'll fall into the freezing water.\n",
      "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
      "    you navigate across the lake and retrieve the disc.\n",
      "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
      "    The surface is described using a grid like the following\n",
      "\n",
      "        SFFF\n",
      "        FHFH\n",
      "        FFFH\n",
      "        HFFG\n",
      "\n",
      "    S : starting point, safe\n",
      "    F : frozen surface, safe\n",
      "    H : hole, fall to your doom\n",
      "    G : goal, where the frisbee is located\n",
      "\n",
      "    The episode ends when you reach the goal or fall in a hole.\n",
      "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(env.env.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно среда представляет собой поле 4 на 4, по которому нужно добраться от начала (клетка *S*) до цели (клетка *G*). При этом среда является недетерменированный - с определенной вероятностью при совершения действия агент подскользнется и попадет не в ту клетку, в которую направлялся. Клетка *H* обозначает прорубь. Игра закначивается, когда агент попадает в клетку *G* или в клету *H*. Если агент проваливается в прорубь, то он получает награду *0*, если достигает клетки цели - *1*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько в среднем очков награды за 100 эпизодов получит наш агент, если будет выполнять случайные действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(0); # from gym.spaces import prng; prng.seed(10) # установим сид для воспроизводимости результатов эксперимента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = []\n",
    "for episode in range(100):\n",
    "    episode_reward = 0\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            print(\"Episode {} finished after {} timesteps\".format(episode+1, t+1))\n",
    "            break\n",
    "    total_reward.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, только в 3 эпизодах из 100 агену удалось добраться до цели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # take a random action\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из среды OpenAI Gym мы можем получить элементы MDP (Markov Decision Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В env.env.P хранится двухуровневый словарь, в котором первый ключ является состояние, а второй - действием.\n",
    "Клетки ассоциированыс индексами [0, 1, 2, ..., 15] слева направо и сверху вниз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(16).reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Индексы действией [0, 1, 2, 3] соответствуют движению на Запад, Юг, Восток и Север.\n",
    "env.env.P[state][action] возвращает лист кортежей (probability, nextstate, reward). Например, состояние 0 - это начальное состояние и информация о веротностях перехода для s=0 и a=0 содержит:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 4, 0.0, False)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другой пример - состояние 5 сооветсвует проруби, и все действия в данном состоянии приводят к тому же состоянию с вероятностью 1 и наградой 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[5][0] = [(1.0, 5, 0, True)]\n",
      "P[5][1] = [(1.0, 5, 0, True)]\n",
      "P[5][2] = [(1.0, 5, 0, True)]\n",
      "P[5][3] = [(1.0, 5, 0, True)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(\"P[5][%i] =\" % i, env.env.P[5][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что из себя представляет алгоритм Value Iteration <img src=\"./value_iteration.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание считается решенным, если агент доходит до цели в среднем в 70% эпизодов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 16\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "n_states = env.env.nS\n",
    "n_actions = env.env.nA\n",
    "print(\"Number of states: {}\".format(n_states))\n",
    "print(\"Number of actions: {}\".format(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем несклько вспомогательных функций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку алгоритм Value Iteration возвращает нам оптимальную V-функцию, то нам необходимо извлекать из нее оптимальную политику (как указано в последней строке псевдокода алгоритма)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_extract_policy(env, v, gamma=1.0, extra_target_state_reward=0.):\n",
    "    n_states = env.nS\n",
    "    n_actions = env.nA\n",
    "    policy = np.zeros(n_states)\n",
    "\n",
    "    for state in range(n_states):\n",
    "        q = np.zeros(n_actions)\n",
    "        for action in range(n_actions):\n",
    "            psrd = np.array(env.P[state][action]) # [[probability], [state], [reward], [done]]\n",
    "            q[action] = np.sum(psrd[:,0]*(\n",
    "                psrd[:,2] + gamma*v[psrd[:,1].astype(np.int)] + psrd[:,3]*extra_target_state_reward))\n",
    "        policy[state] = np.argmax(q)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также напишем функцию для оценки нашей найденной политики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_evaluate_policy(env, policy, gamma=1.0, n=100):\n",
    "    total_reward = []\n",
    "    for episode in range(n):\n",
    "        episode_reward = 0\n",
    "        observation = env.reset()\n",
    "        step = 0\n",
    "        for _ in range(100):\n",
    "            env.render()\n",
    "            action = int(policy[observation])\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            episode_reward += (gamma**step)*reward\n",
    "            step += 1\n",
    "            if done:\n",
    "                break\n",
    "        total_reward.append(episode_reward)\n",
    "    return np.mean(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам остается написать основную функцию, которая вернет оптимальную V-функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iterations=10000, gamma=1.0, extra_target_state_reward=0., epsilon=1e-4):\n",
    "    n_states = env.nS\n",
    "    n_actions = env.nA    \n",
    "    v = np.zeros(n_states)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        delta = np.zeros(n_states)\n",
    "        for state in range(n_states):\n",
    "            u = v[state]\n",
    "            q = np.zeros(n_actions)\n",
    "            for action in range(n_actions):\n",
    "                psrd = np.array(env.P[state][action]) # [[probability], [state], [reward], [done]]\n",
    "                q[action] = np.sum(psrd[:,0]*(\n",
    "                    psrd[:,2] + gamma*v[psrd[:,1].astype(np.int)] + psrd[:,3]*extra_target_state_reward))\n",
    "            v[state] = np.max(q)\n",
    "            delta[state] = np.max((delta[state], np.abs(u - v[state])))\n",
    "#             print(f'[value_iteration] v[{state}]={v[state]:.3f} delta={delta}')\n",
    "        if np.all(delta < epsilon):\n",
    "            break\n",
    "\n",
    "    policy = vi_extract_policy(env, v, gamma, extra_target_state_reward)\n",
    "    return (v, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем найти оптимальную V-функцию, извлечь из нее оптимальную политику и оцениь ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_v, optimal_policy = value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82182145 0.82126109 0.82087163 0.82067347]\n",
      " [0.82199325 0.         0.52824715 0.        ]\n",
      " [0.82226231 0.82260733 0.76389785 0.        ]\n",
      " [0.         0.88171208 0.94085038 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_v.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 3. 3. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_score = vi_evaluate_policy(env, optimal_policy, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сравнению со \"случайным\" агентом, который доходил до цели в 3 случаях из 100, наша новая политика позволяет добирться до цели в ~70% эпизодов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что из себя представляет алгоритм Policy Iteration <img src=\"policy_iteration.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем необходимые вспомогательные функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с основного цикла алгоритма, который вернет нам оптимальную политику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, max_iterations=100, gamma=1.0, extra_target_state_reward=0., epsilon=10e-4):\n",
    "    n_states = env.nS\n",
    "    n_actions = env.nA\n",
    "    v = np.zeros(n_states)\n",
    "    policy = np.random.choice(n_actions, size=(n_states))  # initialize a random policy\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = pi_compute_policy_v(env, policy, max_iterations, gamma, extra_target_state_reward, epsilon)\n",
    "        new_policy = pi_extract_policy(env, old_policy_v, gamma, extra_target_state_reward)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print (f'Policy-Iteration converged at step {i+1}')\n",
    "            break\n",
    "        policy = new_policy\n",
    "    \n",
    "    return (old_policy_v, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А также еще раз напишем функцию для оценки найденной политики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_evaluate_policy(env, policy, gamma=1.0, n=100):\n",
    "    total_reward = []\n",
    "    for episode in range(n):\n",
    "        episode_reward = 0\n",
    "        observation = env.reset()\n",
    "        step = 0\n",
    "        for _ in range(100):\n",
    "            env.render() \n",
    "            action = int(policy[observation])\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            episode_reward += (gamma**step)*reward\n",
    "            step += 1\n",
    "            if done:\n",
    "#                 print(f'[pi_evaluate_policy] done@{step}')\n",
    "                break\n",
    "        total_reward.append(episode_reward)\n",
    "    return np.mean(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остается написать 2 функции, которые используются в основном цикле алгоритма Policy Iteration согласно псевдокоду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_compute_policy_v(env, policy, max_iterations, gamma=1.0, extra_target_state_reward=0., epsilon=1e-4):\n",
    "    n_states = env.nS\n",
    "    v = np.zeros(n_states)\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        delta = np.zeros(n_states)\n",
    "        for state in range(n_states):\n",
    "            u = v[state]\n",
    "            psrd = np.array(env.P[state][policy[state]]) # [[probability], [state], [reward], [done]]\n",
    "            v[state] = np.sum(psrd[:,0]*(\n",
    "                psrd[:,2] + gamma*v[psrd[:,1].astype(np.int)] + psrd[:,3]*extra_target_state_reward))\n",
    "            delta[state] = np.max((delta[state], np.abs(u - v[state])))\n",
    "#             print(f'[pi_compute_policy_v] v[{state}]={v[state]:.3f}\\nv=\\n{v.reshape(4, 12)}\\ndelta=\\n{delta.reshape(4, 12)}')\n",
    "        if np.all(delta < epsilon):\n",
    "            break\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_extract_policy(env, v, gamma=1.0, extra_target_state_reward=0.):\n",
    "    n_states = env.nS\n",
    "    n_actions = env.nA\n",
    "    policy = np.zeros(n_states)\n",
    "\n",
    "    for state in range(n_states):\n",
    "        q = np.zeros(n_actions)\n",
    "        for action in range(n_actions):\n",
    "            psrd = np.array(env.P[state][action]) # [[probability], [state], [reward], [done]]\n",
    "            q[action] = np.sum(psrd[:,0]*(\n",
    "                psrd[:,2] + gamma*v[psrd[:,1].astype(np.int)] + psrd[:,3]*extra_target_state_reward))\n",
    "        policy[state] = np.argmax(q)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы также можем найти оптимальную V-функцию, извлечь из нее оптимальную политику и оцениь ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 4\n"
     ]
    }
   ],
   "source": [
    "optimal_v, optimal_policy = policy_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 3. 3. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_score = pi_evaluate_policy(env, optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительное задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнить поведение случайного агента с обученным вышеприведенными методами агентом в средах из OpenAI Gym с известной динамикой <a href=\"https://gym.openai.com/envs/Taxi-v2/\">Taxi-v2</a> и CliffWalking. Посмотрите как ведет себя агент при использовании разных значений фактора дисконтирования. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ко всем средам OpenAi Gym можно получить доступ не только через *gym.make* но и через обычный импорт модулей. Среда CliffWalking ипортируется именно так. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _CliffWalking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs import toy_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = toy_text.CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This is a simple implementation of the Gridworld Cliff\n",
      "    reinforcement learning task.\n",
      "\n",
      "    Adapted from Example 6.6 (page 106) from Reinforcement Learning: An Introduction\n",
      "    by Sutton and Barto:\n",
      "    http://incompleteideas.net/book/bookdraft2018jan1.pdf\n",
      "\n",
      "    With inspiration from:\n",
      "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
      "\n",
      "    The board is a 4x12 matrix, with (using Numpy matrix indexing):\n",
      "        [3, 0] as the start at bottom-left\n",
      "        [3, 11] as the goal at bottom-right\n",
      "        [3, 1..10] as the cliff at bottom-center\n",
      "\n",
      "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n",
      "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенность данной среды состоит в том, что в ней отсутствует явная награда за переход в терминальное состояние `[3, 11]`. А поскольку факт того, что состояние является терминальным, никак не учитывается в оригинальной версии обоих алгоритмов, это приводит к тому, что они не пытаются \"вести\" игрока в состояние `[3, 11]`.\n",
    "Для решения данной проблемы я решил ввести дополнительный параметр `extra_target_state_reward` - дополнительная награда за переход в терминальное состояние, и учитывать 4-й элемент списка `env.P[state][action]` - флаг `done`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_v, optimal_policy = value_iteration(env, gamma=gamma, extra_target_state_reward=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy.reshape(4,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Динамика среды детерминированная, поэтому достаточно одного прогона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimal_policy_score = vi_evaluate_policy(env, optimal_policy, gamma=gamma, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.0\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По мере уменьшения $\\gamma$ сходится относительно быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 15\n",
      "Wall time: 2.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimal_v, optimal_policy = policy_iteration(env, gamma=gamma, extra_target_state_reward=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy.reshape(4, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 14\n",
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimal_v, optimal_policy = policy_iteration(env, gamma=gamma, extra_target_state_reward=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy.reshape(4, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimal_policy_score = pi_evaluate_policy(env, optimal_policy, gamma=gamma, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.733158334409895\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Taxi-v2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = toy_text.TaxiEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The Taxi Problem\n",
      "    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
      "    by Tom Dietterich\n",
      "\n",
      "    Description:\n",
      "    There are four designated locations in the grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drive to the passenger's location, pick up the passenger, drive to the passenger's destination (another one of the four specified locations), and then drop off the passenger. Once the passenger is dropped off, the episode ends.\n",
      "\n",
      "    Observations: \n",
      "    There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is the taxi), and 4 destination locations. \n",
      "    \n",
      "    Actions: \n",
      "    There are 6 discrete deterministic actions:\n",
      "    - 0: move south\n",
      "    - 1: move north\n",
      "    - 2: move east \n",
      "    - 3: move west \n",
      "    - 4: pickup passenger\n",
      "    - 5: dropoff passenger\n",
      "    \n",
      "    Rewards: \n",
      "    There is a reward of -1 for each action and an additional reward of +20 for delievering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
      "    \n",
      "\n",
      "    Rendering:\n",
      "    - blue: passenger\n",
      "    - magenta: destination\n",
      "    - yellow: empty taxi\n",
      "    - green: full taxi\n",
      "    - other letters (R, G, B and Y): locations for passengers and destinations\n",
      "\n",
      "    actions:\n",
      "    - 0: south\n",
      "    - 1: north\n",
      "    - 2: east\n",
      "    - 3: west\n",
      "    - 4: pickup\n",
      "    - 5: dropoff\n",
      "\n",
      "    state space is represented by:\n",
      "        (taxi_row, taxi_col, passenger_location, destination)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь особенность среды в том, что при $\\gamma = 1$ алгоритмы сходятся плохо, поэтому желательно выбирать $\\gamma < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_v, optimal_policy_vi = value_iteration(env, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 3. 3. 3. 3.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 4. 4. 4. 4. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 5. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 1. 2. 0. 0.\n",
      " 1. 1. 1. 1. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 1. 2. 0. 0. 3. 3. 3. 3.\n",
      " 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 3. 2. 0. 0. 3. 3. 3. 3. 2. 2. 2. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 3. 2. 0. 0. 3. 3. 3. 3. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 3. 1. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 2. 2.\n",
      " 1. 2. 0. 2. 1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3. 2. 2. 2. 2. 1. 2. 3. 2.\n",
      " 1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3. 2. 2. 2. 2. 1. 2. 3. 2. 1. 1. 1. 1.\n",
      " 2. 2. 2. 2. 3. 3. 3. 3. 0. 0. 0. 0. 1. 2. 3. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3. 3. 3. 3. 0. 0. 0. 0. 1. 1. 3. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2.\n",
      " 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 1. 1. 0. 0. 0. 0. 1. 2. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 4. 4. 4. 4. 1. 1. 1. 1. 1. 1. 5. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 1. 1.\n",
      " 2. 2. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 1. 1. 4. 4. 4. 4. 1. 2. 1. 5.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 3. 3. 3. 1. 1. 1. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_score = vi_evaluate_policy(env, optimal_policy_vi, gamma=gamma, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.892240737240751\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 15\n"
     ]
    }
   ],
   "source": [
    "optimal_v, optimal_policy_pi = policy_iteration(env, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 5.,\n",
       "       0., 0., 0., 3., 3., 3., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 2., 2., 2., 2., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 2., 2., 2., 2.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 4.,\n",
       "       4., 4., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 5., 0., 0., 1., 1.,\n",
       "       1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 0.,\n",
       "       0., 1., 1., 1., 1., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 2., 0., 0., 3., 3., 3., 3., 2., 2., 2., 2., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 3., 2., 0., 0., 3., 3., 3., 3., 2., 2., 2., 2., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 3., 2., 0., 0., 3., 3., 3., 3., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       2., 2., 2., 2., 0., 0., 0., 0., 2., 2., 2., 2., 1., 2., 0., 2., 1.,\n",
       "       1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3., 2., 2., 2., 2., 1., 2.,\n",
       "       3., 2., 1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3., 2., 2., 2.,\n",
       "       2., 1., 2., 3., 2., 1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.,\n",
       "       0., 0., 0., 0., 1., 2., 3., 0., 1., 1., 1., 1., 1., 1., 1., 1., 3.,\n",
       "       3., 3., 3., 0., 0., 0., 0., 1., 1., 3., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 2., 2., 2., 2., 1., 1., 1., 1., 2., 2., 2., 2., 1., 2., 1., 2.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 2., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       4., 4., 4., 4., 1., 1., 1., 1., 1., 1., 5., 1., 1., 1., 1., 1., 2.,\n",
       "       2., 2., 2., 1., 1., 1., 1., 2., 2., 2., 2., 1., 2., 1., 2., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 2., 2., 2., 2., 1., 1., 1., 1., 4., 4., 4., 4.,\n",
       "       1., 2., 1., 5., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 3.,\n",
       "       3., 3., 3., 1., 1., 1., 3.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(all(optimal_policy_vi == optimal_policy_pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy_score = pi_evaluate_policy(env, optimal_policy_pi, gamma=gamma, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0451493991495777\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
