{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прекрасный суп\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.486375Z",
     "start_time": "2018-11-08T13:41:00.476531Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть url рассказа О.Генри: http://www.serann.ru/text/poslednii-list-9749\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.757069Z",
     "start_time": "2018-11-08T13:41:00.494266Z"
    }
   },
   "outputs": [],
   "source": [
    "req = requests.get('http://www.serann.ru/text/poslednii-list-9749')\n",
    "req.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.774941Z",
     "start_time": "2018-11-08T13:41:00.760929Z"
    }
   },
   "outputs": [],
   "source": [
    "req.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup. Вы его точно полюбите:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.789626Z",
     "start_time": "2018-11-08T13:41:00.782447Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.840111Z",
     "start_time": "2018-11-08T13:41:00.794594Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.861185Z",
     "start_time": "2018-11-08T13:41:00.846754Z"
    }
   },
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.879680Z",
     "start_time": "2018-11-08T13:41:00.871398Z"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.901204Z",
     "start_time": "2018-11-08T13:41:00.884480Z"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Возможности конечно ограничены версткой, но, в целом, текст после него читать гораздо приятней."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.925502Z",
     "start_time": "2018-11-08T13:41:00.909388Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.949640Z",
     "start_time": "2018-11-08T13:41:00.931416Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.973314Z",
     "start_time": "2018-11-08T13:41:00.957726Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.link.get('href'), soup.link.get('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:00.996204Z",
     "start_time": "2018-11-08T13:41:00.980334Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find_all('link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.026037Z",
     "start_time": "2018-11-08T13:41:01.005199Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find_all('div', {\"class\": \"p\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.052368Z",
     "start_time": "2018-11-08T13:41:01.033254Z"
    }
   },
   "outputs": [],
   "source": [
    "tt = []\n",
    "for t in soup.find_all('div', {\"class\": \"p\"}):\n",
    "    tt.append(t.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.078594Z",
     "start_time": "2018-11-08T13:41:01.060809Z"
    }
   },
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Короткий пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.096484Z",
     "start_time": "2018-11-08T13:41:01.085502Z"
    }
   },
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.117880Z",
     "start_time": "2018-11-08T13:41:01.103394Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.139351Z",
     "start_time": "2018-11-08T13:41:01.123963Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.165492Z",
     "start_time": "2018-11-08T13:41:01.143295Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.192103Z",
     "start_time": "2018-11-08T13:41:01.175066Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.a.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.226007Z",
     "start_time": "2018-11-08T13:41:01.196239Z"
    }
   },
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.attrs)\n",
    "    print(link.get('href'))\n",
    "    print(link.text)\n",
    "    print(link.get('id'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.246452Z",
     "start_time": "2018-11-08T13:41:01.231293Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.275173Z",
     "start_time": "2018-11-08T13:41:01.257339Z"
    }
   },
   "outputs": [],
   "source": [
    "for p in soup.find_all('p'):\n",
    "    print(p.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afisha.ru & links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.939076Z",
     "start_time": "2018-11-08T13:41:01.279389Z"
    }
   },
   "outputs": [],
   "source": [
    "req = requests.get('http://www.afisha.ru')\n",
    "req.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:01.975192Z",
     "start_time": "2018-11-08T13:41:01.947466Z"
    }
   },
   "outputs": [],
   "source": [
    "req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.225661Z",
     "start_time": "2018-11-08T13:41:01.983000Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.285903Z",
     "start_time": "2018-11-08T13:41:02.231208Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.307722Z",
     "start_time": "2018-11-08T13:41:02.292420Z"
    }
   },
   "outputs": [],
   "source": [
    "hrefs = []\n",
    "for a_tag in soup.find_all('a'):\n",
    "        url = a_tag.get('href')\n",
    "        if url and 'http' in url and 'afisha' in url:\n",
    "            hrefs.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.326114Z",
     "start_time": "2018-11-08T13:41:02.311961Z"
    }
   },
   "outputs": [],
   "source": [
    "hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.347809Z",
     "start_time": "2018-11-08T13:41:02.333330Z"
    }
   },
   "outputs": [],
   "source": [
    "hrefs = []\n",
    "for a_tag in soup.find_all('a'):\n",
    "        url = a_tag.get('href')\n",
    "        if url and 'http' in url and 'afisha.ru' in url:\n",
    "            hrefs.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.367533Z",
     "start_time": "2018-11-08T13:41:02.352161Z"
    }
   },
   "outputs": [],
   "source": [
    "hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.388447Z",
     "start_time": "2018-11-08T13:41:02.372943Z"
    }
   },
   "outputs": [],
   "source": [
    "hrefs = []\n",
    "for a_tag in soup.find_all('a'):\n",
    "        url = a_tag.get('href')\n",
    "        if url and 'http' in url and '.afisha.ru' in url:\n",
    "            hrefs.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.406295Z",
     "start_time": "2018-11-08T13:41:02.396599Z"
    }
   },
   "outputs": [],
   "source": [
    "hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.427228Z",
     "start_time": "2018-11-08T13:41:02.409878Z"
    }
   },
   "outputs": [],
   "source": [
    "hrefs = []\n",
    "for a_tag in soup.find_all('a'):\n",
    "        url = a_tag.get('href')\n",
    "        if url and 'http' in url and '.afisha.ru' not in url:\n",
    "            hrefs.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.442788Z",
     "start_time": "2018-11-08T13:41:02.434317Z"
    }
   },
   "outputs": [],
   "source": [
    "hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.464511Z",
     "start_time": "2018-11-08T13:41:02.447616Z"
    }
   },
   "outputs": [],
   "source": [
    "hrefs = []\n",
    "for a_tag in soup.find_all('a'):\n",
    "        url = a_tag.get('href')\n",
    "        if url and 'http' in url and 'afisha' not in url:\n",
    "            hrefs.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.479131Z",
     "start_time": "2018-11-08T13:41:02.469639Z"
    }
   },
   "outputs": [],
   "source": [
    "hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.501251Z",
     "start_time": "2018-11-08T13:41:02.483175Z"
    }
   },
   "outputs": [],
   "source": [
    "pics = soup.find_all('img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.522164Z",
     "start_time": "2018-11-08T13:41:02.507277Z"
    }
   },
   "outputs": [],
   "source": [
    "pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.569253Z",
     "start_time": "2018-11-08T13:41:02.529300Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for src_tag in pics:\n",
    "    print(src_tag[\"src\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:02.883497Z",
     "start_time": "2018-11-08T13:41:02.576333Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "urllib.request.urlretrieve(pics[5]['src'], '5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие примеры спользования find_all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <html>\n",
    "#  <head>\n",
    "#   <title>\n",
    "#    The Dormouse's story\n",
    "#   </title>\n",
    "#  </head>\n",
    "#  <body>\n",
    "#   <p class=\"title\">\n",
    "#    <b>\n",
    "#     The Dormouse's story\n",
    "#    </b>\n",
    "#   </p>\n",
    "#   <p class=\"story\">\n",
    "#    Once upon a time there were three little sisters; and their names were\n",
    "#    <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
    "#     Elsie\n",
    "#    </a>\n",
    "#    ,\n",
    "#    <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
    "#     Lacie\n",
    "#    </a>\n",
    "#    and\n",
    "#    <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link2\">\n",
    "#     Tillie\n",
    "#    </a>\n",
    "#    ; and they lived at the bottom of a well.\n",
    "#   </p>\n",
    "#   <p class=\"story\">\n",
    "#    ...\n",
    "#   </p>\n",
    "#  </body>\n",
    "# </html>\n",
    "\n",
    "soup.find_all(\"title\")\n",
    "# [<title>The Dormouse's story</title>]\n",
    "\n",
    "soup.find_all(\"p\", \"title\")\n",
    "# [<p class=\"title\"><b>The Dormouse's story</b></p>]\n",
    "\n",
    "soup.find_all(\"a\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "soup.find_all(id=\"link2\")\n",
    "# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n",
    "\n",
    "import re\n",
    "soup.find(string=re.compile(\"sisters\"))\n",
    "# u'Once upon a time there were three little sisters; and their names were\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блиц!\n",
    "\n",
    "    \n",
    "1 Возьмите статью Википедии (любую, которая вам понравится). Найдите на ней ссылки на другие статьи. Относительная ссылка на статью выгдядит так: /wiki/название_статьи - без символа '#' (что было бы ссылкой внутри статьи) и символа '.' (что было бы ссылкой на изображение). Выберете 1 случайную ссылку, дополните ее префиксом 'https://ru.wikipedia.org'. Зайдите на получившуюся страницу и распечатайте title.\n",
    "\n",
    "\n",
    "2 Выкачиваем Википедию: \n",
    "\n",
    "Напишите функцию, которая по url переходит на страницу, достает из нее title и сохраняет resp.content в файл {title}.html , такие файлы удобно будет располагать в отдельной папке 'wiki_pages'. После этого функция парсит все ссылки на страницы в Википедии и вызывает себя же для каждой из ссылок.\n",
    "\n",
    "PS: Записать в файл можно через open('{title}.html'.format(title=title), 'wb')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:08:15.380489Z",
     "start_time": "2018-11-12T21:08:15.370336Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('wiki_pages', exist_ok=True)\n",
    " \n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "wisited_urls = set()\n",
    " \n",
    "def href_is_valid_wiki_article(href):\n",
    "    return href and href.startswith('/wiki/') and '#' not in href and '.' not in href\n",
    " \n",
    "def parse_wiki_urls(url, save_page=False):\n",
    "    print(url)\n",
    "    wisited_urls.add(url)\n",
    "   \n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "   \n",
    "    if save_page:\n",
    "        with open('wiki_pages/' + '{}.html'.format(soup.title.text).replace('/', '-'), 'wb') as fp:\n",
    "            fp.write(resp.content)\n",
    "       \n",
    "    for url_tag in soup.find_all('a', href=href_is_valid_wiki_article):\n",
    "        url = 'https://ru.wikipedia.org' + url_tag['href']\n",
    "        if url not in wisited_urls:\n",
    "            parse_wiki_urls(url, save_page=save_page)\n",
    "\n",
    "parse_wiki_urls('https://ru.wikipedia.org/wiki/1I/Оумуамуа', save_page=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telegram messages & pandas (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:06.868077Z",
     "start_time": "2018-11-08T13:41:02.889148Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:08.783964Z",
     "start_time": "2018-11-08T13:41:06.880175Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"telegram_messages.html\"), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:08.790953Z",
     "start_time": "2018-11-08T13:41:08.786551Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:09.132963Z",
     "start_time": "2018-11-08T13:41:08.795650Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = soup.find_all('div', attrs={\"class\": \"message default clearfix\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:09.147086Z",
     "start_time": "2018-11-08T13:41:09.135966Z"
    }
   },
   "outputs": [],
   "source": [
    "print(messages[0].find('div', \n",
    "                       attrs={\"class\": \"pull_right date details\"}).attrs)\n",
    "print(messages[0].find('div', \n",
    "                       attrs={\"class\": \"pull_right date details\"}).attrs['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:09.163203Z",
     "start_time": "2018-11-08T13:41:09.151321Z"
    }
   },
   "outputs": [],
   "source": [
    "print(messages[0].find('div', attrs={\"class\":\"from_name\"}).attrs)\n",
    "print(messages[0].find('div', attrs={\"class\":\"from_name\"}).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:09.177385Z",
     "start_time": "2018-11-08T13:41:09.166994Z"
    }
   },
   "outputs": [],
   "source": [
    "print(messages[0].find('div', attrs={\"class\":\"text\"}).attrs)\n",
    "print(messages[0].find('div', attrs={\"class\":\"text\"}).get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:09.189310Z",
     "start_time": "2018-11-08T13:41:09.182530Z"
    }
   },
   "outputs": [],
   "source": [
    "for link in messages[0].find('div', attrs={\"class\":\"text\"}).find_all('a'):\n",
    "    print(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:41:59.209454Z",
     "start_time": "2018-11-08T13:41:59.200140Z"
    }
   },
   "outputs": [],
   "source": [
    "[str(td).strip() for td in messages[0].find('div', attrs={\"class\":\"text\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:42:49.307247Z",
     "start_time": "2018-11-08T13:42:48.260562Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ddi = []\n",
    "for message in messages:\n",
    "    di = {}\n",
    "    di['date'] = message.find('div', attrs={\"class\":\"pull_right date details\"}).attrs['title']\n",
    "    di['name'] = message.find('div', attrs={\"class\":\"from_name\"}).get_text(strip=True)\n",
    "    try:\n",
    "        di['text_all'] = message.find('div', attrs={\"class\":\"text\"}).text.replace('$', '\\$')\n",
    "    except AttributeError:\n",
    "        di['text_all'] = np.nan\n",
    "    try:\n",
    "        for link_index, link in enumerate(message.find('div', attrs={\"class\":\"text\"}).find_all('a')):\n",
    "            di['link_' + str(link_index).zfill(2)] = link.text\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        di['text'] = ' '.join([sent.replace('$', '\\$') for sent in [str(td).strip() \\\n",
    "                   for td in message.find('div', \\\n",
    "                   attrs={\"class\":\"text\"})] if not sent.startswith('<a href=') and len(sent) > 0])\n",
    "        di['source'] = di['text'].split(':')[0]\n",
    "        di['text'] = ' '.join(di['text'].split(':')[1:]).strip()\n",
    "        di['company'] = ' '.join(re.findall(\"([A-Za-z]+[\\d@]+[\\w@]*|[\\d@]+[A-Za-z]+[\\w@]*|[A-Za-z]+[\\w@]*)\", \n",
    "                                               di['text']))\n",
    "    except:\n",
    "        di['text'] = np.nan\n",
    "        di['source'] = np.nan\n",
    "        \n",
    "    ddi.append(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:42:50.500667Z",
     "start_time": "2018-11-08T13:42:50.445990Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(ddi)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T13:42:51.546744Z",
     "start_time": "2018-11-08T13:42:50.825764Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, ax = plt.subplots(figsize=(25,15))\n",
    "sns_heatmap = sns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='OrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
